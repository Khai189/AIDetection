{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsIpuWHF1TwhKaRVMHdswb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khai189/AIDetection/blob/main/AI_Non_AI_Images_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction: AI Image Detector\n",
        "\n",
        "This is the EDA, model preperation, and experimentation behind my web extension for AI Image Detection.\n",
        "\n",
        "I trained the model off a kaggle dataset, and the original model is from `torchvision.models.EfficientNetB0`, where you can find the documentation."
      ],
      "metadata": {
        "id": "Edwt8-UTQo9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Data Setup"
      ],
      "metadata": {
        "id": "e7OK3XKTQ6XE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the data from kaggle"
      ],
      "metadata": {
        "id": "6VXUZKQYQ8ys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgz-m4ZzAELJ"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "from pathlib import Path\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ayushmandatta1/deepdetect-2025\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e79b292b"
      },
      "source": [
        "source_path = path\n",
        "\n",
        "destination_path = '/content/kaggle_dataset'\n",
        "\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "# Get all items in the source directory\n",
        "items_in_source = os.listdir(source_path)\n",
        "\n",
        "print(f\"Contents of the downloaded directory ({source_path}): {items_in_source}\")\n",
        "\n",
        "train_dir_processed = '/content/train'\n",
        "test_dir_processed = '/content/test'\n",
        "\n",
        "# Check if the processed data directories already exist\n",
        "if os.path.exists(train_dir_processed) and os.path.exists(test_dir_processed) and os.listdir(train_dir_processed) != []:\n",
        "\n",
        "  print(f\"Dataset already exists, skipping data creation\")\n",
        "\n",
        "else:\n",
        "  for item in items_in_source:\n",
        "      s = os.path.join(source_path, item)\n",
        "      d = os.path.join(destination_path, item)\n",
        "      print(f\"Copying {s} to {d}\")\n",
        "\n",
        "      if os.path.isdir(s):\n",
        "          # For directories, use copytree. It requires the destination to not exist.\n",
        "          if os.path.exists(d):\n",
        "              print(f\"Destination directory {d} already exists, removing it before copy.\")\n",
        "              shutil.rmtree(d)\n",
        "          shutil.copytree(s, d)\n",
        "      else:\n",
        "          # For files, use copy2 (which preserves metadata)\n",
        "          shutil.copy2(s, d)\n",
        "\n",
        "  print(f\"Dataset copied to: {destination_path}\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the test and train directories (if needed)"
      ],
      "metadata": {
        "id": "Zg8403rVRBD2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61910fdb"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define base dirs\n",
        "data_dir = '/content/kaggle_dataset/ddata'\n",
        "train_dir = '/content/train'\n",
        "test_dir = '/content/test'\n",
        "\n",
        "if os.path.exists(train_dir_processed) and os.path.exists(test_dir_processed) and os.listdir(train_dir_processed) != []:\n",
        "  print(f\"Dataset already exists, skipping prep\")\n",
        "else:\n",
        "\n",
        "  os.makedirs(train_dir, exist_ok=True)\n",
        "  os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "  classes = ['fake', 'real']\n",
        "\n",
        "  for cls in classes:\n",
        "      os.makedirs(os.path.join(train_dir, cls), exist_ok=True)\n",
        "      os.makedirs(os.path.join(test_dir, cls), exist_ok=True)\n",
        "\n",
        "  print(f\"Created training directories: {os.listdir(train_dir)}\")\n",
        "  print(f\"Created testing directories: {os.listdir(test_dir)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the testing and training data"
      ],
      "metadata": {
        "id": "vAhYmdb4RHMY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "188e9ecf"
      },
      "source": [
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "test_size = 0.2\n",
        "\n",
        "if os.path.exists(train_dir_processed) and os.path.exists(test_dir_processed) and os.listdir(train_dir_processed) != []:\n",
        "  print(f\"Dataset already exists, skipping prep\")\n",
        "else:\n",
        "  for cls in classes:\n",
        "      source_class_dir = os.path.join(data_dir, cls)\n",
        "\n",
        "      all_files = [os.path.join(source_class_dir, f) for f in os.listdir(source_class_dir) if os.path.isfile(os.path.join(source_class_dir, f))]\n",
        "\n",
        "      train_files, test_files = train_test_split(all_files, test_size=test_size, random_state=42)\n",
        "\n",
        "      print(f\"\\nClass: {cls}\")\n",
        "      print(f\"Total files: {len(all_files)}\")\n",
        "      print(f\"Train files: {len(train_files)}\")\n",
        "      print(f\"Test files: {len(test_files)}\")\n",
        "\n",
        "      dest_train_class_dir = os.path.join(train_dir, cls)\n",
        "      for f_path in train_files:\n",
        "          shutil.copy(f_path, dest_train_class_dir)\n",
        "\n",
        "      dest_test_class_dir = os.path.join(test_dir, cls)\n",
        "      for f_path in test_files:\n",
        "          shutil.copy(f_path, dest_test_class_dir)\n",
        "\n",
        "\n",
        "  # Verify counts\n",
        "  print(\"\\nVerification:\")\n",
        "  for cls in classes:\n",
        "      print(f\"Train {cls} count: {len(os.listdir(os.path.join(train_dir, cls)))}\")\n",
        "      print(f\"Test {cls} count: {len(os.listdir(os.path.join(test_dir, cls)))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n"
      ],
      "metadata": {
        "id": "IJIt-XaUBjsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "jkE_ipwQHCiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up DataLoaders and Datasets for our initial model"
      ],
      "metadata": {
        "id": "4CzKXu6-RLtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "train_dir_abs = \"/content/train\"\n",
        "test_dir_abs = \"/content/test\"\n",
        "\n",
        "checkpoint_dir_train = os.path.join(train_dir_abs, '.ipynb_checkpoints')\n",
        "checkpoint_dir_test = os.path.join(test_dir_abs, '.ipynb_checkpoints')\n",
        "\n",
        "if os.path.exists(checkpoint_dir_train):\n",
        "    shutil.rmtree(checkpoint_dir_train)\n",
        "    print(f\"Removed: {checkpoint_dir_train}\")\n",
        "\n",
        "if os.path.exists(checkpoint_dir_test):\n",
        "    shutil.rmtree(checkpoint_dir_test)\n",
        "    print(f\"Removed: {checkpoint_dir_test}\")\n",
        "\n",
        "train_dir = Path(train_dir_abs)\n",
        "test_dir = Path(test_dir_abs)\n",
        "\n",
        "weights = models.EfficientNet_B0_Weights.DEFAULT\n",
        "auto_transforms = weights.transforms()\n",
        "\n",
        "try:\n",
        "    train_data = ImageFolder(train_dir,\n",
        "                             transform=auto_transforms)\n",
        "\n",
        "    test_data = ImageFolder(test_dir,\n",
        "                            transform=auto_transforms)\n",
        "    print(\"ImageFolder initialization successful!\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error initializing ImageFolder: {e}\")\n",
        "    print(\"Please ensure 'train' and 'test' directories contain 'FAKE' and 'REAL' subdirectories with image files.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "h1HAnaihBty8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes"
      ],
      "metadata": {
        "id": "InUap0c7IcaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data), class_names"
      ],
      "metadata": {
        "id": "l_VnmbGXFd-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "train_dataloader = DataLoader(train_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              num_workers=NUM_WORKERS,\n",
        "                              shuffle=True,\n",
        "                              pin_memory=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              num_workers=NUM_WORKERS,\n",
        "                              shuffle=False,\n",
        "                              pin_memory=True)"
      ],
      "metadata": {
        "id": "bNwIsR8hHuJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "DW5Pho_wI2ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape, label.shape"
      ],
      "metadata": {
        "id": "ygmU8K7mI5LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at an example from our data to visualize"
      ],
      "metadata": {
        "id": "-Rh2y_NcRS1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(image[0].cpu().permute(1, 2, 0))\n",
        "plt.title(class_names[label[0]])"
      ],
      "metadata": {
        "id": "qEll5QxUI6yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Freeze the model and look at the parameters for our data"
      ],
      "metadata": {
        "id": "Cz_Su90sQkqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_model = models.efficientnet_b0(weights=weights).to(device)\n",
        "for param in initial_model.features.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "W6A2LWvyJCyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_model.classifier"
      ],
      "metadata": {
        "id": "sXTvogT-Rxhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import torchinfo\n",
        "except:\n",
        "  !pip install torchinfo\n",
        "\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "I028_lOhQJ6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(initial_model,\n",
        "        input_size=(32, 3, 224, 224),\n",
        "        col_names=[\"trainable\", \"num_params\", \"output_size\"],\n",
        "        col_width=20)"
      ],
      "metadata": {
        "id": "4PgV0t_sQQvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=.2, inplace=True),\n",
        "    nn.Linear(in_features=1280, out_features=len(class_names))\n",
        ")"
      ],
      "metadata": {
        "id": "uwbigF9vRfsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i8qlWqRMoCDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0u1ufNdRod5w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}